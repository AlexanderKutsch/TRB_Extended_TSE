{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d302be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon, LineString\n",
    "import contextily as cx\n",
    "import datetime as dt\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from functions_file import *\n",
    "print(testfunction(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a6276",
   "metadata": {},
   "source": [
    "## Process data\n",
    "First, reformat data from MSs and LDs for all polygons. Then gather and save the data in one file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1766280e",
   "metadata": {},
   "source": [
    "### 1. Reformat MS and LD data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e4c861",
   "metadata": {},
   "source": [
    "**Key settings for reformatting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e13cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons = pd.read_csv('../data/polygons11.csv')\n",
    "polygons = polygons.drop([4])\n",
    "polygons['polygon'] = polygons.name\n",
    "poly_cols = ['polygon','lanes','direction','busstops','seplane','length','complexity','road_rank']\n",
    "\n",
    "savefactors = {}\n",
    "scalefactorsfile = 'scalefactors_bypolygon.pkl' # 'scalefactors_bypolygon_bymode.pkl'\n",
    "\n",
    "# LD and MS data should have same resampling\n",
    "resample = '30s' \n",
    "window = '3T' # 3min\n",
    "polygon_names = polygons.name.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7305304",
   "metadata": {},
   "source": [
    "**Mobile sensor data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6eed1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# Resample, scale and gather data for mobile sensor modes\n",
    "#################\n",
    "save = 'off'\n",
    "\n",
    "for polygon_name in polygon_names:\n",
    "    \n",
    "    POLYGON = get_polygon(polygon_name,polygons)\n",
    "    # 1. load 'regular' and 'car' data\n",
    "    # 1.a. file 1 - regular\n",
    "    nonresampled = import_sensor_data(POLYGON['name'],'MS')\n",
    "    modes        = set(nonresampled['mode'])\n",
    "    resampled    = sensor_resample_window(nonresampled,modes,resample=resample,window=window,POLYGON=POLYGON,plot='off',filna=False)\n",
    "    #display(nonresampled[nonresampled['mode']=='all'].head())\n",
    "    #display(resampled[resampled['mode']=='all'].head())\n",
    "    # 1.b. file 2 - car penetration rates\n",
    "    nonresampled_cars = import_sensor_data(POLYGON['name'],'MS',cars='Yes')\n",
    "    modes_cars        = set(nonresampled_cars['mode'])\n",
    "    resampled_cars    = sensor_resample_window(nonresampled_cars,modes_cars,resample=resample,window=window,POLYGON=POLYGON,plot='off',filna=False)\n",
    "    # 1.c. combine data\n",
    "    data = pd.concat([resampled,resampled_cars])\n",
    "    \n",
    "    # 2. normalize/scale by link\n",
    "    cols       = ['densities','flows']\n",
    "    xx         = data[data['mode']=='all'][cols]\n",
    "    means      = xx.mean()\n",
    "    data[cols] = data[cols]/xx.mean() \n",
    "    interval   = data[(data['mode']=='all')&(data.densities>0.2)&(data.densities<2)]\n",
    "    slope      = interval.flows.mean()/interval.densities.mean()\n",
    "    data.flows = data.flows/slope\n",
    "    savefactors[polygon_name+'_MS'] = [1,slope,means[0],means[1]]\n",
    "    \n",
    "    # 3. reformat - create a df per mode, rename columns, merge\n",
    "    result = []\n",
    "    for mode in set(data['mode']):\n",
    "        # can exclude exp_id - not needed & issues due to diff. experiment start/end times\n",
    "        tmp = data[data['mode']==mode][['exp_id','polygon','DOW','times','speeds','densities','flows']]\n",
    "        tmp = tmp.rename(columns={'speeds':'v_%s'%(mode),'densities':'k_%s'%(mode),'flows':'q_%s'%(mode)})\n",
    "        if (len(result)<1):\n",
    "            result = tmp\n",
    "        else:\n",
    "            result = pd.merge(result, tmp, on=['polygon','DOW','times','exp_id'],how='outer')\n",
    "    \n",
    "    # 4. add polygon information\n",
    "    result = pd.merge(result, polygons[poly_cols], how='outer', on=['polygon'])\n",
    "    display(result.head(2))\n",
    "    \n",
    "    # 5. save\n",
    "    if save=='on':\n",
    "        result.to_pickle('../output/data_processed/processed_data_MS_%s.pkl'%(POLYGON['name']))\n",
    "        print('Saved %s.'%(polygon_name))\n",
    "    print('Length:',len(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eb17a2",
   "metadata": {},
   "source": [
    "**Loop detector data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a2f02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# Resampe, scale and gather data for 'all' using loop detector approach\n",
    "#################\n",
    "save = 'off'\n",
    "\n",
    "for polygon_name in polygon_names:\n",
    "    \n",
    "    POLYGON = get_polygon(polygon_name,polygons)\n",
    "    # 1. load 'all' from 'regular' data\n",
    "    nonresampled = import_sensor_data(POLYGON['name'],sensor='LD')\n",
    "    modes        = set(nonresampled['mode'])\n",
    "    resampled    = sensor_resample_window(nonresampled,modes,resample=resample,window=window,POLYGON=POLYGON,plot='off')\n",
    "    data         = resampled\n",
    "    \n",
    "    # 2. normalize/scale by link\n",
    "    xx = data[data['mode']=='all'][['densities','flows']]\n",
    "    means    = xx.mean()\n",
    "    data[['densities','flows']] = data[['densities','flows']]/xx.mean() # dont need exp_id or speeds\n",
    "    interval = data[(data['mode']=='all')&(data.densities>0.2)&(data.densities<2)]\n",
    "    slope    = interval.flows.mean()/interval.densities.mean()\n",
    "    savefactors[polygon_name+'_LD'] = [1,slope,means[0],means[1]]\n",
    "    \n",
    "    # 3. reformat - create a df per mode, rename columns, merge\n",
    "    result = []\n",
    "    for mode in set(data['mode']):\n",
    "        tmp = data[data['mode']==mode][['exp_id','polygon','DOW','times','speeds','densities','flows']]\n",
    "        tmp = tmp.rename(columns={'speeds':'v_%s'%(mode),'densities':'k_%s'%(mode),'flows':'q_%s'%(mode)})\n",
    "        if (len(result)<1):\n",
    "            result = tmp\n",
    "        else:\n",
    "            result = pd.merge(result, tmp, on=['polygon','DOW','times','exp_id'],how='outer')\n",
    "    \n",
    "    # 4. add polygon information\n",
    "    result = pd.merge(result, polygons[poly_cols], how='outer', on=['polygon'])\n",
    "    display(result.head(2))\n",
    "    \n",
    "    # 5. save\n",
    "    if save=='on':\n",
    "        result.to_pickle('../output/data_processed/processed_data_LD_%s.pkl'%(POLYGON['name'])) # index=False#with parked\n",
    "        print('Saved.')\n",
    "    print('Length:',len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95afd298",
   "metadata": {},
   "source": [
    "#### Save scalefactors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14191ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save scalefactors\n",
    "with open(scalefactorsfile, 'wb') as f:\n",
    "    pickle.dump(savefactors, f)    \n",
    "# to read\n",
    "with open(scalefactorsfile, 'rb') as f:\n",
    "    scalefactors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5197de55",
   "metadata": {},
   "source": [
    "### 2. Gather all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74938e0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save = 'off'\n",
    "\n",
    "# 1. load and merge data\n",
    "all_MS = pd.concat( [pd.read_pickle('../output/data_processed/processed_data_MS_%s.pkl'%(p)) for p in polygons.name.values] )\n",
    "all_LD = pd.concat( [pd.read_pickle('../output/data_processed/processed_data_LD_%s.pkl'%(p)) for p in polygons.name.values] )\n",
    "vqk_cols_MS = [col for col in all_MS if col.startswith(('q_','v_','k_'))]\n",
    "vqk_cols_LD = [col for col in all_LD if col.startswith(('q_','v_','k_'))]\n",
    "all_MS.columns = [a+'_MS' if a in vqk_cols_MS else a for a in all_MS.columns]\n",
    "all_LD.columns = [a+'_LD' if a in vqk_cols_LD else a for a in all_LD.columns]\n",
    "non_vqk_cols = [col for col in all_MS if not col.startswith(('q_','v_','k_'))]\n",
    "all_data = pd.merge(all_MS, all_LD, how='outer',on=non_vqk_cols,suffixes = (\"_MS\",\"_LD\"))\n",
    "\n",
    "# 2.a. set tiny values to zero\n",
    "vqk_cols = [col for col in all_data if col.startswith(('q_','v_','k_'))]\n",
    "for c in vqk_cols:\n",
    "    tmp = all_data[c]\n",
    "    tmp = [0 if a<1e-10 else a for a in tmp]\n",
    "    all_data[c] = tmp\n",
    "# 2.b. drop where ALL of [v,q,k] in [0,nan] --> either no traffic or no footage because in between recordings\n",
    "leninit = len(all_data)\n",
    "all_data = all_data.loc[~(all_data[vqk_cols].isin([0,np.nan])).all(axis=1)]\n",
    "print('%.1f%% removed due to ambiguity (no traffic or no data?). New length: %s.'%(100*(1-len(all_data)/leninit),len(all_data)))\n",
    "\n",
    "# 3. replace q and k NaNs with 0s\n",
    "all_q_k_cols = [col for col in all_data if col.startswith(('q_','k_'))]\n",
    "all_data[all_q_k_cols] = all_data[all_q_k_cols].fillna(0)\n",
    "print('\\nNaNs:\\n',all_data[[col for col in all_data if col.startswith(('q_','v_','k_'))]].isna().sum())\n",
    "\n",
    "# 4. save\n",
    "if save=='on':\n",
    "    #all_data.to_pickle('../output/processed_data/processed_data_all_bypolygonandmode.pkl') # index=False\n",
    "    all_data.to_pickle('../output/data_processed/processed_data_all_bypolygon.pkl') # index=False\n",
    "    print('Saved.')\n",
    "display(all_data.head(3))\n",
    "print('Length:',len(all_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92896920",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### 3. Check some stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5a7379",
   "metadata": {},
   "source": [
    "#### Check q-k plot of normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db4fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = all_data \n",
    "\n",
    "# allMS data\n",
    "plt.figure(figsize=(9,6))\n",
    "for i in [0,1,2,3,5,6,7,8,9,10,11]:\n",
    "    subset = data[data.polygon=='polygon_r%s'%i]\n",
    "    plt.scatter(subset.k_all_MS,subset.q_all_MS,s=8,alpha=0.3,label=subset.polygon.values[0])\n",
    "\n",
    "plt.title('All mobile sensor data, interval length: %s, rolling window: %s'%(resample,window))\n",
    "plt.xlabel('Density k (scale converted)'); plt.ylabel('Flow q (scale converted)'); plt.legend()\n",
    "#plt.savefig('All_MS_scaled.png')\n",
    "plt.show()\n",
    "\n",
    "# allLD data\n",
    "plt.figure(figsize=(9,6))\n",
    "for i in [0,1,2,3,5,6,7,8,9,10,11]:\n",
    "    subset = data[data.polygon=='polygon_r%s'%i]\n",
    "    plt.scatter(subset.k_all_LD,subset.q_all_LD,s=8,alpha=0.3,label=subset.polygon.values[0])\n",
    "\n",
    "plt.title('All virtual loop detector data')\n",
    "plt.xlabel('Density k (scale converted)');plt.ylabel('Flow q (scale converted)'); plt.legend()\n",
    "\n",
    "plt.show()\n",
    "print(len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e352e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
