{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3135985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon, LineString\n",
    "import contextily as cx\n",
    "import datetime as dt\n",
    "import json\n",
    "\n",
    "from functions_file import *\n",
    "print(testfunction(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86e0d4",
   "metadata": {},
   "source": [
    "### Outline\n",
    "1. Import data and add basic information\n",
    "    1. Import data\n",
    "    1. Define area & Coarse filtering\n",
    "    1. Add information\n",
    "    1. Fine filtering\n",
    "2. Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1f4099",
   "metadata": {},
   "source": [
    "### 1. Import data and add basic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf5426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# IMPORT DATA\n",
    "##################\n",
    "\n",
    "veh_info_list_all = pd.read_csv('../data/veh_info_list.csv',sep=',')\n",
    "waypoints_w_dist_all = pd.read_csv('../data/waypoints_w_dist.csv')\n",
    "# make unique id\n",
    "waypoints_w_dist_all['unique_id'] = waypoints_w_dist_all.exp_id*1000000+waypoints_w_dist_all.track_id\n",
    "veh_info_list_all['unique_id'] = veh_info_list_all.exp_id*1000000+veh_info_list_all.track_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35c700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_info = pd.read_csv('../data/experiment_list_info.csv')\n",
    "# remove exp_ids in exp_info that don't appear in selected region\n",
    "exp_info = exp_info[exp_info.exp_id.isin(waypoints_w_dist_all.exp_id)]\n",
    "# merge exp info\n",
    "tmp = pd.merge(exp_info, waypoints_w_dist_all, how=\"outer\", on=[\"exp_id\"])\n",
    "# map modes to waypoints_w_dist\n",
    "waypoints_all = pd.merge(veh_info_list_all[['unique_id','type']], tmp, how=\"outer\", on=[\"unique_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd321bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# SELECT ROAD SEGMENT / POLYGON\n",
    "# All with bus route. Direction is driving direction.\n",
    "# Repeat from here on for every polygon: starting from polygon 0 to polygon 11, except polygon 4 (which contains several links)\n",
    "##################\n",
    "\n",
    "polygon_name = 'polygon_r0'\n",
    "\n",
    "polygons = pd.read_csv('../data/polygons11.csv')\n",
    "POLYGON = get_polygon(polygon_name,polygons)\n",
    "print('Available polygon names:',[a for a in polygons.name.values])\n",
    "print('\\nName: %s\\nLength (km): %s\\nLanes: %i\\nDirection (Â°): %s\\nBus stops: %i\\nSeparate lane: %i\\nComment: %s\\nCoordinates: %s'%(\n",
    "      POLYGON['name'],POLYGON['length'],POLYGON['lanes'],POLYGON['direction'],POLYGON['busstops'],POLYGON['seplane'],POLYGON['comment'],POLYGON['coords']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832c43a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# COARSE FILTERING\n",
    "##################\n",
    "\n",
    "lons,lats = POLYGON['coords'].exterior.coords.xy\n",
    "waypoints_w_dist = waypoints_w_dist_all[\n",
    "    (waypoints_w_dist_all.lat>min(lats)) &\n",
    "    (waypoints_w_dist_all.lat<max(lats)) &\n",
    "    (waypoints_w_dist_all.lon>min(lons)) &\n",
    "    (waypoints_w_dist_all.lon<max(lons)) ]\n",
    "\n",
    "# remove ids in veh_info_list that don't appear in selected region\n",
    "veh_info_list = veh_info_list_all[veh_info_list_all.unique_id.isin(waypoints_w_dist.unique_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc5e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# ADD INFORMATION\n",
    "##################\n",
    "\n",
    "# import info\n",
    "exp_info = pd.read_csv('../data/experiment_list_info.csv')\n",
    "# remove exp_ids in exp_info that don't appear in selected region\n",
    "exp_info = exp_info[exp_info.exp_id.isin(waypoints_w_dist.exp_id)]\n",
    "\n",
    "# merge exp info\n",
    "tmp = pd.merge(exp_info, waypoints_w_dist, how=\"outer\", on=[\"exp_id\"])\n",
    "# map modes to waypoints_w_dist\n",
    "waypoints_w_dist_mode = pd.merge(veh_info_list[['unique_id','type']], tmp, how=\"outer\", on=[\"unique_id\"])\n",
    "\n",
    "# make a global time\n",
    "timestamps = []\n",
    "hours = waypoints_w_dist_mode.ST//1\n",
    "start_minutes = np.where(waypoints_w_dist_mode.ST%1>0, 30, 0)\n",
    "minutes = start_minutes + waypoints_w_dist_mode.time//60\n",
    "DOYs =  waypoints_w_dist_mode.DOY\n",
    "seconds = waypoints_w_dist_mode.time%60\n",
    "for i in range(len(waypoints_w_dist_mode)):\n",
    "    timestamps.append(dt.datetime.strptime(\n",
    "        '%s %i:%i:%f'%(DOYs[i],hours[i],minutes[i],seconds[i]),'%Y-%m-%d %H:%M:%S.%f'))\n",
    "waypoints_w_dist_mode['timestamp'] = timestamps  \n",
    "\n",
    "# drop unnecessary columns\n",
    "waypoints_w_dist_mode.drop(columns=['DOY']) # 'DOW'\n",
    "\n",
    "# check\n",
    "print(waypoints_w_dist_mode.head())\n",
    "print(waypoints_w_dist_mode['lat'].isna().sum()) # check if all were assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49b6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# FINE FILTERING\n",
    "##################\n",
    "\n",
    "non_geo_cols = list(waypoints_w_dist_mode.columns.drop(['lat','lon']))\n",
    "geo_df_full = gpd.GeoDataFrame(waypoints_w_dist_mode[non_geo_cols], crs='epsg:4326', \n",
    "                          geometry=gpd.points_from_xy(waypoints_w_dist_mode.lon, waypoints_w_dist_mode.lat))\n",
    "waypoints_w_dist_mode = geo_df_full[geo_df_full.geometry.within(POLYGON['coords'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c394f2",
   "metadata": {},
   "source": [
    "### 2. Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e363030",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# CLEAN DATA 1\n",
    "##################\n",
    "\n",
    "# remove cyclists and pedestrians\n",
    "waypoints_w_dist_mode = waypoints_w_dist_mode[~waypoints_w_dist_mode['type'].isin(['Bicycle','Pedestrian'])]\n",
    "\n",
    "# remove start and end\n",
    "leninit = len(waypoints_w_dist_mode)\n",
    "remove = 60 #s\n",
    "max_time_exp = waypoints_w_dist_mode.groupby(by=['exp_id']).max()['time']\n",
    "for exp in max_time_exp.index:\n",
    "    # drop first and last seconds\n",
    "    x = waypoints_w_dist_mode[\n",
    "        (waypoints_w_dist_mode.exp_id==exp) &\n",
    "        ((waypoints_w_dist_mode.time<remove) | (waypoints_w_dist_mode.time>max_time_exp[exp]-remove))]\n",
    "    waypoints_w_dist_mode = waypoints_w_dist_mode.drop(x.index.values)\n",
    "print('Start/end removed: %.1f%%. New length is %s.\\n'%(100*(1-len(waypoints_w_dist_mode)/leninit),len(waypoints_w_dist_mode)))\n",
    "\n",
    "# check for speed outliers\n",
    "print('Max. speed of ')\n",
    "for exp in max_time_exp.index:\n",
    "    subset = waypoints_w_dist_mode[(waypoints_w_dist_mode.exp_id==exp)]\n",
    "    print('mean: exp %i: %.ikm/h'%(exp,subset.speed.mean()))\n",
    "    \n",
    "# remove experiment 8\n",
    "remove = [8]\n",
    "waypoints_w_dist_mode = waypoints_w_dist_mode[~waypoints_w_dist_mode.exp_id.isin(remove)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def4f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove parked vehicles\n",
    "group_df = pd.DataFrame()\n",
    "group_df[['unique_id','d1']] = waypoints_w_dist_mode.groupby('unique_id')['trv_dist'].max().reset_index()[['unique_id','trv_dist']]\n",
    "group_df['d0'] = waypoints_w_dist_mode.groupby('unique_id')['trv_dist'].min().reset_index()['trv_dist']\n",
    "group_df['ddiff'] = group_df.d1-group_df.d0\n",
    "display(group_df.head())\n",
    "cutoff_distance = POLYGON['length']*1000 / 10\n",
    "remove = group_df[group_df.ddiff<cutoff_distance].unique_id.values\n",
    "waypoints_w_dist_mode = waypoints_w_dist_mode[~waypoints_w_dist_mode.unique_id.isin(remove)]\n",
    "print('%.1f%% of probes cover less than %s of the %s m'%(100*len(group_df[group_df.ddiff<cutoff_distance])/len(group_df),cutoff_distance,POLYGON['length']*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7480e2a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# CLEAN DATA 2 # remove vehicles moving in wrong direction\n",
    "##################\n",
    "\n",
    "# get start and end location\n",
    "idx_max = waypoints_w_dist_mode.groupby(['unique_id'])['time'].transform(max) == waypoints_w_dist_mode['time']\n",
    "idx_min = waypoints_w_dist_mode.groupby(['unique_id'])['time'].transform(min) == waypoints_w_dist_mode['time']\n",
    "ids = waypoints_w_dist_mode[idx_max].unique_id.values\n",
    "data_max = waypoints_w_dist_mode[idx_max].geometry.reset_index(drop=True)\n",
    "data_min = waypoints_w_dist_mode[idx_min].geometry.reset_index(drop=True)\n",
    "# check each vehicle\n",
    "remove = []\n",
    "correct = POLYGON['direction']\n",
    "ds = []\n",
    "for idx,i in enumerate(ids):\n",
    "    loc1 = data_max.iloc[idx]\n",
    "    loc0 = data_min.loc[idx]\n",
    "    th = np.arctan2(loc1.y-loc0.y, loc1.x-loc0.x);\n",
    "    direction = (th*180/3.14 + 360) % 360; \n",
    "    \n",
    "    if abs(direction-correct)>30:\n",
    "        print('id: %s, direction: %s, correct direction:%s'%(i,direction,correct))\n",
    "        remove.append(i)\n",
    "    else:\n",
    "        ds.append(direction)\n",
    "print('Average direction: %s, correct direction: %s'%(np.mean(ds),correct))\n",
    "# remove faulty vehicle ids\n",
    "leninit = len(waypoints_w_dist_mode)\n",
    "waypoints_w_dist_mode = waypoints_w_dist_mode[~waypoints_w_dist_mode.unique_id.isin(remove)]\n",
    "print('removed %.1f%%.'%(100*(1-len(waypoints_w_dist_mode)/leninit)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4161625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# SAVE DATA\n",
    "##################\n",
    "\n",
    "# COLUMNS: 'unique_id','type','exp_id','DOW','ST','ET','DOY','track_id','time','speed','trv_dist','timestamp','geometry'\n",
    "\n",
    "save = 'pickle' # 'pickle','csv'\n",
    "if save=='csv':\n",
    "    print(waypoints_w_dist_mode.head())\n",
    "    file_name = '../output/data_clean/prepared_data_%s.csv'%(POLYGON['name']) \n",
    "    waypoints_w_dist_mode.to_csv(file_name,encoding='iso-8859-1',index=False) \n",
    "    print('Saved as csv.')\n",
    "elif save=='pickle':\n",
    "    file_name = '../output/data_clean/prepared_data_%s.pkl'%(POLYGON['name']) \n",
    "    waypoints_w_dist_mode.to_pickle(file_name) # index=False\n",
    "    print('Saved as pickle.')\n",
    "else:\n",
    "    print('Not saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0211066a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# IMPORT DATA\n",
    "#########################\n",
    "polygon_name = 'polygon_r0'\n",
    "file_type = 'pickle'\n",
    "if file_type=='pickle':\n",
    "    file_name = '../output/data_clean/prepared_data_%s.pkl'%(polygon_name)\n",
    "if file_type=='csv':\n",
    "    file_name = '../output/data_clean/prepared_data_%s.csv'%(polygon_name)\n",
    "\n",
    "waypoints_w_dist_mode = import_clean_data(file_name,file_type,polygon_name)\n",
    "display(waypoints_w_dist_mode.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67afd0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
